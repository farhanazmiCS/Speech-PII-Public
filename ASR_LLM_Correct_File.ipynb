{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking PII identification from Speech\n",
    "\n",
    "The goal of this experiment is to evaluate the efficacy of various prompt-engineering techniques for PII identification. Previously, we employed an entity-aware ASR model—fine-tuned on Singlish speech and enhanced with an expanded tokenizer—to perform NER tagging directly on speech. In that setup, the LLM correction module was tasked with addressing both transcription errors and PII-tagging errors, potentially limiting its ability to focus solely on improving PII detection.\n",
    "\n",
    "In this experiment, we will use the fine-tuned ASR model (trained on Singlish dialects) with its default tokenizer and delegate the entire PII tagging task to an LLM. This approach allows us to systematically compare different LLM prompting methods to determine which yields the best performance for our PII identification objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Login Hugging Face CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Perform transcription with N-best using fine-tuned ASR\n",
    "\n",
    "Skip this step if already transcribed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Download the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available, using CUDA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"CUDA is available, using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"MPS is available, using MPS\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA and MPS are not available, switching to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb0a52bc3c54058ad9488abc6bde29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facfa3c9346b46eaaa4bb7dd0ea767a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b02bc14e61049e7b4e7bc999c8f4a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4a2cfdd1294eba9f1d9c96e8444bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc17ec67e8b457d9aabb1f7ab13eabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c1079b06bc4fd2a47571987bf233a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9549ddcccb4669bfc71b710266047f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a206c0e1c8148968160e7da96ea36d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6b5a52b9214a52b736b8d5a74b72a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc7493068e14af380175a32ff56771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293e038504d94128b2e30686093e6b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/1.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small.en\") # Using the default feature extractor and tokenizer\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"f-azm17/whisper-small_en_seed_gretel_similar0.3\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Load the dataset\n",
    "\n",
    "For this example, we shall use the 150 dataset in the test set from `Audio_Files_for_testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_key(file: str) -> int:\n",
    "    try:\n",
    "        # 3 digit\n",
    "        key = int(file[2:5])\n",
    "    except ValueError:\n",
    "        # 1 digit\n",
    "        if file[3] == '.':\n",
    "            key = int(file[2])\n",
    "        else:\n",
    "            key = int(file[2:4])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "audio_files = sorted(os.listdir(\"Audio_Files_for_testing\"), key=retrieve_key)\n",
    "audio_files = [f'Audio_Files_for_testing/{file}' for file in audio_files]\n",
    "print(len(audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Files_for_testing/id1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Files_for_testing/id2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Files_for_testing/id3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Files_for_testing/id4.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Files_for_testing/id5.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name\n",
       "0  Audio_Files_for_testing/id1.wav\n",
       "1  Audio_Files_for_testing/id2.wav\n",
       "2  Audio_Files_for_testing/id3.wav\n",
       "3  Audio_Files_for_testing/id4.wav\n",
       "4  Audio_Files_for_testing/id5.wav"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.DataFrame(data=audio_files, columns=['file_name'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from typing import List\n",
    "\n",
    "def transcribe(audioPath: str, model: AutoModelForSpeechSeq2Seq, processor: AutoProcessor, device: str, best_n: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    A function which transcribes the audio based on a given audio file path.\n",
    "    Outputs the transcript along with the identified PII entities.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    audioPath (str) -- The path to the audio\\n\n",
    "    model (AutoModelForSpeechSeq2Seq) -- The ASR model\\n\n",
    "    processor (AutoProcessor) -- The processor, which contains the feature extractor and tokenizer.\\n\n",
    "    best_n (int) -- The best n number. By default, return the best transcription. \n",
    "\n",
    "    Return: The transcription along with the identified PII entities. (str)\n",
    "    \"\"\"\n",
    "    waveform, sr = librosa.load(audioPath, sr=16000)\n",
    "    inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_features=inputs[\"input_features\"], \n",
    "            temperature=1.0,\n",
    "            num_beams=best_n,\n",
    "            num_return_sequences=best_n\n",
    "        )\n",
    "    transcriptions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing and Identifying PII from test set...: 100%|██████████| 150/150 [03:17<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), desc=\"Transcribing and Identifying PII from test set...\", total=len(test_df)):\n",
    "    transcriptions = transcribe(row['file_name'], model, processor, device, 5)\n",
    "    for i, transcription in enumerate(transcriptions):\n",
    "        test_df.at[index, f'rank_{i+1}'] = transcription  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "      <th>rank_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Files_for_testing/id1.wav</td>\n",
       "      <td>the day before yesterday ram received another ...</td>\n",
       "      <td>The day before yesterday, jason received anoth...</td>\n",
       "      <td>The day before yesterday, Ram received another...</td>\n",
       "      <td>The day before yesterday,ram received another ...</td>\n",
       "      <td>The day before yesterday RAM received another ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Files_for_testing/id2.wav</td>\n",
       "      <td>um My date of birth is uh 2 september  19 92</td>\n",
       "      <td>mm my date of birth is uhm 2 september ninetee...</td>\n",
       "      <td>My date of birth is uh second september 19 92</td>\n",
       "      <td>My date of birth is 2 september,   9092 H</td>\n",
       "      <td>mm My date of birth is  uh second september ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Files_for_testing/id3.wav</td>\n",
       "      <td>hmm She handed over a crumpled piece of paper ...</td>\n",
       "      <td>she handed over a crumpled piece of paper ther...</td>\n",
       "      <td>She handed over a crumpled piece of paper  Thi...</td>\n",
       "      <td>She handed over a crumpled piece of paper ther...</td>\n",
       "      <td>she handed over a crumpled piece of paper  for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Files_for_testing/id4.wav</td>\n",
       "      <td>uh and uh three of the other one ya</td>\n",
       "      <td>okay and uh three three of the other one yeah ...</td>\n",
       "      <td>I'll be picking it with another one and uh thr...</td>\n",
       "      <td>and uh uuh three three of the other ones yeah</td>\n",
       "      <td>uh and uh  uh  3  3 of the other one yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Files_for_testing/id5.wav</td>\n",
       "      <td>uh  Hong 's EMAIL  is  P X 1R z at    47 at  ...</td>\n",
       "      <td>uhhh   Hong  s email is  px 1 rzu 4 7 at yahoo...</td>\n",
       "      <td>Hong's email is  P x1 rz'a 47 at yahoo.com</td>\n",
       "      <td>hongs email is  P x 1 r z a 4 7 at yahoo dot com</td>\n",
       "      <td>hes saying hes still  px one rz a four seven ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name  \\\n",
       "0  Audio_Files_for_testing/id1.wav   \n",
       "1  Audio_Files_for_testing/id2.wav   \n",
       "2  Audio_Files_for_testing/id3.wav   \n",
       "3  Audio_Files_for_testing/id4.wav   \n",
       "4  Audio_Files_for_testing/id5.wav   \n",
       "\n",
       "                                              rank_1  \\\n",
       "0  the day before yesterday ram received another ...   \n",
       "1      um My date of birth is uh 2 september  19 92    \n",
       "2  hmm She handed over a crumpled piece of paper ...   \n",
       "3                uh and uh three of the other one ya   \n",
       "4   uh  Hong 's EMAIL  is  P X 1R z at    47 at  ...   \n",
       "\n",
       "                                              rank_2  \\\n",
       "0  The day before yesterday, jason received anoth...   \n",
       "1  mm my date of birth is uhm 2 september ninetee...   \n",
       "2  she handed over a crumpled piece of paper ther...   \n",
       "3  okay and uh three three of the other one yeah ...   \n",
       "4  uhhh   Hong  s email is  px 1 rzu 4 7 at yahoo...   \n",
       "\n",
       "                                              rank_3  \\\n",
       "0  The day before yesterday, Ram received another...   \n",
       "1      My date of birth is uh second september 19 92   \n",
       "2  She handed over a crumpled piece of paper  Thi...   \n",
       "3  I'll be picking it with another one and uh thr...   \n",
       "4        Hong's email is  P x1 rz'a 47 at yahoo.com    \n",
       "\n",
       "                                              rank_4  \\\n",
       "0  The day before yesterday,ram received another ...   \n",
       "1         My date of birth is 2 september,   9092 H    \n",
       "2  She handed over a crumpled piece of paper ther...   \n",
       "3      and uh uuh three three of the other ones yeah   \n",
       "4  hongs email is  P x 1 r z a 4 7 at yahoo dot com    \n",
       "\n",
       "                                              rank_5  \n",
       "0  The day before yesterday RAM received another ...  \n",
       "1  mm My date of birth is  uh second september ni...  \n",
       "2  she handed over a crumpled piece of paper  for...  \n",
       "3          uh and uh  uh  3  3 of the other one yeah  \n",
       "4   hes saying hes still  px one rz a four seven ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('whisper-small_en_seed_gretel_similar0.3_no_tag_test_set_transcribed_n_best_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4 (Optional): Load the transcribed files, if already transcribed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "      <th>rank_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Files_for_testing/id1.wav</td>\n",
       "      <td>the day before yesterday ram received another ...</td>\n",
       "      <td>The day before yesterday, jason received anoth...</td>\n",
       "      <td>The day before yesterday, Ram received another...</td>\n",
       "      <td>The day before yesterday,ram received another ...</td>\n",
       "      <td>The day before yesterday RAM received another ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Files_for_testing/id2.wav</td>\n",
       "      <td>um My date of birth is uh 2 september  19 92</td>\n",
       "      <td>mm my date of birth is uhm 2 september ninetee...</td>\n",
       "      <td>My date of birth is uh second september 19 92</td>\n",
       "      <td>My date of birth is 2 september,   9092 H</td>\n",
       "      <td>mm My date of birth is  uh second september ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Files_for_testing/id3.wav</td>\n",
       "      <td>hmm She handed over a crumpled piece of paper ...</td>\n",
       "      <td>she handed over a crumpled piece of paper ther...</td>\n",
       "      <td>She handed over a crumpled piece of paper  Thi...</td>\n",
       "      <td>She handed over a crumpled piece of paper ther...</td>\n",
       "      <td>she handed over a crumpled piece of paper  for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Files_for_testing/id4.wav</td>\n",
       "      <td>uh and uh three of the other one ya</td>\n",
       "      <td>okay and uh three three of the other one yeah ...</td>\n",
       "      <td>I'll be picking it with another one and uh thr...</td>\n",
       "      <td>and uh uuh three three of the other ones yeah</td>\n",
       "      <td>uh and uh  uh  3  3 of the other one yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Files_for_testing/id5.wav</td>\n",
       "      <td>uh  Hong 's EMAIL  is  P X 1R z at    47 at  ...</td>\n",
       "      <td>uhhh   Hong  s email is  px 1 rzu 4 7 at yahoo...</td>\n",
       "      <td>Hong's email is  P x1 rz'a 47 at yahoo.com</td>\n",
       "      <td>hongs email is  P x 1 r z a 4 7 at yahoo dot com</td>\n",
       "      <td>hes saying hes still  px one rz a four seven ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name  \\\n",
       "0  Audio_Files_for_testing/id1.wav   \n",
       "1  Audio_Files_for_testing/id2.wav   \n",
       "2  Audio_Files_for_testing/id3.wav   \n",
       "3  Audio_Files_for_testing/id4.wav   \n",
       "4  Audio_Files_for_testing/id5.wav   \n",
       "\n",
       "                                              rank_1  \\\n",
       "0  the day before yesterday ram received another ...   \n",
       "1      um My date of birth is uh 2 september  19 92    \n",
       "2  hmm She handed over a crumpled piece of paper ...   \n",
       "3                uh and uh three of the other one ya   \n",
       "4   uh  Hong 's EMAIL  is  P X 1R z at    47 at  ...   \n",
       "\n",
       "                                              rank_2  \\\n",
       "0  The day before yesterday, jason received anoth...   \n",
       "1  mm my date of birth is uhm 2 september ninetee...   \n",
       "2  she handed over a crumpled piece of paper ther...   \n",
       "3  okay and uh three three of the other one yeah ...   \n",
       "4  uhhh   Hong  s email is  px 1 rzu 4 7 at yahoo...   \n",
       "\n",
       "                                              rank_3  \\\n",
       "0  The day before yesterday, Ram received another...   \n",
       "1      My date of birth is uh second september 19 92   \n",
       "2  She handed over a crumpled piece of paper  Thi...   \n",
       "3  I'll be picking it with another one and uh thr...   \n",
       "4        Hong's email is  P x1 rz'a 47 at yahoo.com    \n",
       "\n",
       "                                              rank_4  \\\n",
       "0  The day before yesterday,ram received another ...   \n",
       "1         My date of birth is 2 september,   9092 H    \n",
       "2  She handed over a crumpled piece of paper ther...   \n",
       "3      and uh uuh three three of the other ones yeah   \n",
       "4  hongs email is  P x 1 r z a 4 7 at yahoo dot com    \n",
       "\n",
       "                                              rank_5  \n",
       "0  The day before yesterday RAM received another ...  \n",
       "1  mm My date of birth is  uh second september ni...  \n",
       "2  she handed over a crumpled piece of paper  for...  \n",
       "3          uh and uh  uh  3  3 of the other one yeah  \n",
       "4   hes saying hes still  px one rz a four seven ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('whisper-small_en_seed_gretel_similar0.3_no_tag_test_set_transcribed_n_best_5.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform ASR correction with N-best and in-context learning\n",
    "\n",
    "We will now need to generate the best (corrected) transcription based on the 5-best list generated by the ASR. We will leverage the in-context learning (ICL) approach proposed by Hyporadise with zero-shot learning to perform the ASR correction.\n",
    "\n",
    "The model used in the Hyporadise paper was GPT-3.5. As with the advancements to large language models and AI, the LLaMA-3.1-8b models have surpassed GPT-3.5 in many benchmarks, which can be seen in this link: https://www.vellum.ai/comparison/gpt-3-5-turbo-vs-llama-3-1-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Perform ASR correction with LLaMa\n",
    "\n",
    "We shall use the Pipeline version to get the corrected ASR transcription, as the manual tokenizer + model approach seems to be simply outputting the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-shot in-context learning (As per the Hyporadise Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_domain = \"conversational speech containing personal identifiable information\"\n",
    "\n",
    "one_shot_example = {\n",
    "    \"hypotheses\": [\n",
    "        \"um My date of birth is uh 2 september  19 92\",\n",
    "        \"mm my date of birth is uhm 2 september nineteen ninety-two\",\n",
    "        \"My date of birth is uh second september 19 92\",\n",
    "        \"My date of birth is 2 september,   9092 H\",\n",
    "        \"mm My date of birth is  uh second september nineteen ninety-two\"\n",
    "    ],\n",
    "    \"expected_output\": \"um My date of birth is uh 2nd september 1992\"\n",
    "}\n",
    "\n",
    "formatted_example_hypotheses = \"\\n\".join([f\"{i+1}: {hypothesis}\" for i, hypothesis in enumerate(one_shot_example[\"hypotheses\"])])\n",
    "\n",
    "actual_hypotheses = []\n",
    "\n",
    "questions = [\n",
    "    \"Are you familiar with speech recognition?\",\n",
    "    \"Are you familiar with language model rescoring in ASR?\",\n",
    "    \"Can you give a possible example on language model rescoring with 5-best hypotheses?\",\n",
    "    f\"\"\"\n",
    "        Nice job, I will give you an example as a demonstration from {target_domain}. \n",
    "        The five best hypotheses list is:\n",
    "        {formatted_example_hypotheses}\n",
    "        \n",
    "        I expect your output to be: {one_shot_example[\"expected_output\"]}\n",
    "        \n",
    "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf710ff4c8ce4ff4a8018c9fb96bae1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "        ### SYSTEM PROMPT ###\n",
      "        You are selecting the best ASR transcription.\n",
      "        \n",
      "        RULES (Guidelines, But Selection is Mandatory):\n",
      "        - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
      "        - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
      "        - Ignore capitalization differences.\n",
      "        - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
      "        - If no single transcription follows all these rules, select the **closest match**.\n",
      "        - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
      "        \n",
      "        ---\n",
      "        ### QUESTION ###\n",
      "        Select the best transcription from the following:\n",
      "        Hypothesis 1: the day before yesterday ram received another email from  R e m y at outlook.sg \n",
      "Hypothesis 2: The day before yesterday, jason received another email from  R e m y at outlook.sg \n",
      "Hypothesis 3: The day before yesterday, Ram received another email from  R e m y at outlook.sg  \n",
      "Hypothesis 4: The day before yesterday,ram received another email from  r e e n y at outlook.sg \n",
      "Hypothesis 5: The day before yesterday RAM received another email from  R e m y at outlook.sg \n",
      "        ---\n",
      "        ### ANSWER ###\n",
      "        ANSWER:\n",
      "    \n",
      "### QUESTION ###\n",
      "\n",
      "        Nice job, I will give you an example as a demonstration from conversational speech containing personal identifiable information. \n",
      "        The five best hypotheses list is:\n",
      "        1: um My date of birth is uh 2 september  19 92\n",
      "2: mm my date of birth is uhm 2 september nineteen ninety-two\n",
      "3: My date of birth is uh second september 19 92\n",
      "4: My date of birth is 2 september,   9092 H\n",
      "5: mm My date of birth is  uh second september nineteen ninety-two\n",
      "        \n",
      "        I expect your output to be: um My date of birth is uh 2nd september 1992\n",
      "        \n",
      "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
      "    \n",
      "Hypothesis 1: okay so there is no charges however like what I mentioned just now you need to maintain a minimum of huh $10,000 for your bank account\n",
      "Hypothesis 2: okay so there is no charges, however like what I mentioned just now you need to maintain a minimum of  10k for your bank account\n",
      "Hypothesis 3: okay so there is no charges however like what I mentioned just now you need to maintain a minimum of a  10k  for your bank account\n",
      "Hypothesis 4: okay so there is no charges however like what I've mentioned just now you need to maintain a minimum of  uh  10k  for your bank account \n",
      "Hypothesis 5: ok so there is no charges however like what I mentioned just now you need to maintain a minimum of   10 k for your bank account \n",
      "\n",
      "Response:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import transformers\n",
    "\n",
    "# Load LLaMA 8B pipeline (uses bf16 for lower memory)\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Get the first row from test_df as a Series\n",
    "some_row = test_df.iloc[22]\n",
    "\n",
    "for i, question in enumerate(questions[3:]):\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "        ### SYSTEM PROMPT ###\n",
    "        You are selecting the best ASR transcription.\n",
    "        \n",
    "        RULES (Guidelines, But Selection is Mandatory):\n",
    "        - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
    "        - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
    "        - Ignore capitalization differences.\n",
    "        - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
    "        - If no single transcription follows all these rules, select the **closest match**.\n",
    "        - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
    "        \n",
    "        ---\n",
    "        ### QUESTION ###\n",
    "        Select the best transcription from the following:\n",
    "        {formatted_hypotheses}\n",
    "        ---\n",
    "        ### ANSWER ###\n",
    "        ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve actual hypotheses for ASR correction\n",
    "    actual_hypotheses = list(some_row[['rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5']])\n",
    "    formatted_hypotheses = \"\\n\".join([f\"Hypothesis {i+1}: {hypothesis}\" for i, hypothesis in enumerate(actual_hypotheses)])\n",
    "    full_prompt = f\"{system_prompt}\\n### QUESTION ###\\n{question}\\n{formatted_hypotheses}\"\n",
    "\n",
    "    print(\"Prompt:\\n\")\n",
    "    print(full_prompt)\n",
    "\n",
    "    # Generate response using the LLaMA pipeline\n",
    "    response = pipeline(\n",
    "        full_prompt,\n",
    "        max_new_tokens=256,  # Limit output length\n",
    "        min_length=5,\n",
    "        do_sample=False,  # Deterministic response\n",
    "        temperature=0.0,  # Avoid randomness\n",
    "        return_full_text=False,  # Prevents repeating the input prompt\n",
    "        repetition_penalty=1.2\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(response)\n",
    "\n",
    "    # Free GPU memory after each run\n",
    "    del full_prompt, response\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf710ff4c8ce4ff4a8018c9fb96bae1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "        ### SYSTEM PROMPT ###\n",
      "        You are selecting the best ASR transcription.\n",
      "        \n",
      "        RULES (Guidelines, But Selection is Mandatory):\n",
      "        - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
      "        - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
      "        - Ignore capitalization differences.\n",
      "        - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
      "        - If no single transcription follows all these rules, select the **closest match**.\n",
      "        - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
      "        \n",
      "        ---\n",
      "        ### QUESTION ###\n",
      "        Select the best transcription from the following:\n",
      "        Hypothesis 1: the day before yesterday ram received another email from  R e m y at outlook.sg \n",
      "Hypothesis 2: The day before yesterday, jason received another email from  R e m y at outlook.sg \n",
      "Hypothesis 3: The day before yesterday, Ram received another email from  R e m y at outlook.sg  \n",
      "Hypothesis 4: The day before yesterday,ram received another email from  r e e n y at outlook.sg \n",
      "Hypothesis 5: The day before yesterday RAM received another email from  R e m y at outlook.sg \n",
      "        ---\n",
      "        ### ANSWER ###\n",
      "        ANSWER:\n",
      "    \n",
      "### QUESTION ###\n",
      "\n",
      "        Nice job, I will give you an example as a demonstration from conversational speech containing personal identifiable information. \n",
      "        The five best hypotheses list is:\n",
      "        1: um My date of birth is uh 2 september  19 92\n",
      "2: mm my date of birth is uhm 2 september nineteen ninety-two\n",
      "3: My date of birth is uh second september 19 92\n",
      "4: My date of birth is 2 september,   9092 H\n",
      "5: mm My date of birth is  uh second september nineteen ninety-two\n",
      "        \n",
      "        I expect your output to be: um My date of birth is uh 2nd september 1992\n",
      "        \n",
      "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
      "    \n",
      "Hypothesis 1: okay so there is no charges however like what I mentioned just now you need to maintain a minimum of huh $10,000 for your bank account\n",
      "Hypothesis 2: okay so there is no charges, however like what I mentioned just now you need to maintain a minimum of  10k for your bank account\n",
      "Hypothesis 3: okay so there is no charges however like what I mentioned just now you need to maintain a minimum of a  10k  for your bank account\n",
      "Hypothesis 4: okay so there is no charges however like what I've mentioned just now you need to maintain a minimum of  uh  10k  for your bank account \n",
      "Hypothesis 5: ok so there is no charges however like what I mentioned just now you need to maintain a minimum of   10 k for your bank account \n",
      "\n",
      "Response:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import transformers\n",
    "\n",
    "# Load LLaMA 8B pipeline (uses bf16 for lower memory)\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Get the first row from test_df as a Series\n",
    "some_row = test_df.iloc[22]\n",
    "\n",
    "for i, question in enumerate(questions[3:]):\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "        ### SYSTEM PROMPT ###\n",
    "        You are selecting the best ASR transcription.\n",
    "        \n",
    "        RULES (Guidelines, But Selection is Mandatory):\n",
    "        - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
    "        - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
    "        - Ignore capitalization differences.\n",
    "        - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
    "        - If no single transcription follows all these rules, select the **closest match**.\n",
    "        - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
    "        \n",
    "        ---\n",
    "        ### QUESTION ###\n",
    "        Select the best transcription from the following:\n",
    "        {formatted_hypotheses}\n",
    "        ---\n",
    "        ### ANSWER ###\n",
    "        ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve actual hypotheses for ASR correction\n",
    "    actual_hypotheses = list(some_row[['rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5']])\n",
    "    formatted_hypotheses = \"\\n\".join([f\"Hypothesis {i+1}: {hypothesis}\" for i, hypothesis in enumerate(actual_hypotheses)])\n",
    "    full_prompt = f\"{system_prompt}\\n### QUESTION ###\\n{question}\\n{formatted_hypotheses}\"\n",
    "\n",
    "    print(\"Prompt:\\n\")\n",
    "    print(full_prompt)\n",
    "\n",
    "    # Generate response using the LLaMA pipeline\n",
    "    response = pipeline(\n",
    "        full_prompt,\n",
    "        max_new_tokens=256,  # Limit output length\n",
    "        min_length=5,\n",
    "        do_sample=False,  # Deterministic response\n",
    "        temperature=0.0,  # Avoid randomness\n",
    "        return_full_text=False,  # Prevents repeating the input prompt\n",
    "        repetition_penalty=1.2\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(response)\n",
    "\n",
    "    # Free GPU memory after each run\n",
    "    del full_prompt, response\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf710ff4c8ce4ff4a8018c9fb96bae1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "        ### SYSTEM PROMPT ###\n",
      "        You are selecting the best ASR transcription.\n",
      "        \n",
      "        RULES (Guidelines, But Selection is Mandatory):\n",
      "        - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
      "        - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
      "        - Ignore capitalization differences.\n",
      "        - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
      "        - If no single transcription follows all these rules, select the **closest match**.\n",
      "        - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
      "        \n",
      "        ---\n",
      "        ### QUESTION ###\n",
      "        Select the best transcription from the following:\n",
      "        Hypothesis 1: the day before yesterday ram received another email from  R e m y at outlook.sg \n",
      "Hypothesis 2: The day before yesterday, jason received another email from  R e m y at outlook.sg \n",
      "Hypothesis 3: The day before yesterday, Ram received another email from  R e m y at outlook.sg  \n",
      "Hypothesis 4: The day before yesterday,ram received another email from  r e e n y at outlook.sg \n",
      "Hypothesis 5: The day before yesterday RAM received another email from  R e m y at outlook.sg \n",
      "        ---\n",
      "        ### ANSWER ###\n",
      "        ANSWER:\n",
      "    \n",
      "### QUESTION ###\n",
      "\n",
      "        Nice job, I will give you an example as a demonstration from conversational speech containing personal identifiable information. \n",
      "        The five best hypotheses list is:\n",
      "        1: um My date of birth is uh 2 september  19 92\n",
      "2: mm my date of birth is uhm 2 september nineteen ninety-two\n",
      "3: My date of birth is uh second september 19 92\n",
      "4: My date of birth is 2 september,   9092 H\n",
      "5: mm My date of birth is  uh second september nineteen ninety-two\n",
      "        \n",
      "        I expect your output to be: um My date of birth is uh 2nd september 1992\n",
      "        \n",
      "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
      "    \n",
      "Hypothesis 1: okay so there is no charges however like what I mentioned just now you need to maintain a minimum of huh $10,000 for your bank account\n",
      "Hypothesis 2: okay so there is no charges, however like what I mentioned just now you need to maintain a minimum of  10k for your bank account\n",
      "Hypothesis 3: okay so there is no charges however like what I mentioned just now you need to maintain a minimum of a  10k  for your bank account\n",
      "Hypothesis 4: okay so there is no charges however like what I've mentioned just now you need to maintain a minimum of  uh  10k  for your bank account \n",
      "Hypothesis 5: ok so there is no charges however like what I mentioned just now you need to maintain a minimum of   10 k for your bank account \n",
      "\n",
      "Response:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import transformers\n",
    "\n",
    "# Load LLaMA 8B pipeline (uses bf16 for lower memory)\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Get the first row from test_df as a Series\n",
    "some_row = test_df.iloc[22]\n",
    "\n",
    "for i, question in enumerate(questions[3:]):\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "        ### SYSTEM PROMPT ###\n",
    "        You are selecting the best ASR transcription.\n",
    "        \n",
    "        RULES (Guidelines, But Selection is Mandatory):\n",
    "        - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
    "        - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
    "        - Ignore capitalization differences.\n",
    "        - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
    "        - If no single transcription follows all these rules, select the **closest match**.\n",
    "        - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
    "        \n",
    "        ---\n",
    "        ### QUESTION ###\n",
    "        Select the best transcription from the following:\n",
    "        {formatted_hypotheses}\n",
    "        ---\n",
    "        ### ANSWER ###\n",
    "        ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve actual hypotheses for ASR correction\n",
    "    actual_hypotheses = list(some_row[['rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5']])\n",
    "    formatted_hypotheses = \"\\n\".join([f\"Hypothesis {i+1}: {hypothesis}\" for i, hypothesis in enumerate(actual_hypotheses)])\n",
    "    full_prompt = f\"{system_prompt}\\n### QUESTION ###\\n{question}\\n{formatted_hypotheses}\"\n",
    "\n",
    "    print(\"Prompt:\\n\")\n",
    "    print(full_prompt)\n",
    "\n",
    "    # Generate response using the LLaMA pipeline\n",
    "    response = pipeline(\n",
    "        full_prompt,\n",
    "        max_new_tokens=256,  # Limit output length\n",
    "        min_length=5,\n",
    "        do_sample=False,  # Deterministic response\n",
    "        temperature=0.0,  # Avoid randomness\n",
    "        return_full_text=False,  # Prevents repeating the input prompt\n",
    "        repetition_penalty=1.2\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(response)\n",
    "\n",
    "    # Free GPU memory after each run\n",
    "    del full_prompt, response\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f338f62fe2ae4c7cbd8fc33c3ccd0568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "    ### SYSTEM PROMPT ###\n",
      "    You are selecting the best ASR transcription.\n",
      "\n",
      "    RULES (Guidelines, But Selection is Mandatory):\n",
      "    - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
      "    - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
      "    - Ignore capitalization differences.\n",
      "    - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
      "    - If no single transcription follows all these rules, select the **closest match**.\n",
      "    - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
      "\n",
      "    ---\n",
      "    \n",
      "### QUESTION ###\n",
      "\n",
      "        Nice job, I will give you an example as a demonstration from conversational speech containing personal identifiable information. \n",
      "        The five best hypotheses list is:\n",
      "        1: um My date of birth is uh 2 september  19 92\n",
      "2: mm my date of birth is uhm 2 september nineteen ninety-two\n",
      "3: My date of birth is uh second september 19 92\n",
      "4: My date of birth is 2 september,   9092 H\n",
      "5: mm My date of birth is  uh second september nineteen ninety-two\n",
      "        \n",
      "        I expect your output to be: um My date of birth is uh 2nd september 1992\n",
      "        \n",
      "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
      "    \n",
      "Hypothesis 1: the day before yesterday ram received another email from  R e m y at outlook.sg \n",
      "Hypothesis 2: The day before yesterday, jason received another email from  R e m y at outlook.sg \n",
      "Hypothesis 3: The day before yesterday, Ram received another email from  R e m y at outlook.sg  \n",
      "Hypothesis 4: The day before yesterday,ram received another email from  r e e n y at outlook.sg \n",
      "Hypothesis 5: The day before yesterday RAM received another email from  R e m y at outlook.sg \n",
      "\n",
      "### ANSWER ###\n",
      "ANSWER:\n",
      "\n",
      "Response:\n",
      "\n",
      "The day before yesterday, Ram received another email from Remy@outlook.sg\n",
      "\n",
      "\n",
      "### REASONING SKILL FOR CONVERSATIONAL AI TRANSCRIPTION SELECTION AND POST-PROCESSING ###\n",
      "\n",
      "The correct selection requires careful consideration and analysis of each hypothesis based on the provided guidelines:\n",
      "\n",
      "*   **Prefer numeric digits**: Hypotheses 1, 2, 3, and 5 contain \"day\" or \"days,\" which doesn't fit well with our rule for dates; however, we do see some potential in using numerical values like \"the day before yesterday.\" However, none of them use actual numbers that could replace words like \"second.\"\n",
      "*   **Standard email formatting**: All options have variations but only one has it correctly formatted without any extra characters between domain name parts (\"at\").\n",
      "\n",
      "\n",
      "\n",
      "Based on the given examples, the closest match would follow the general structure of being written out instead of having specific numbers used where they should not be. Therefore, considering the available choices, the selected answer must adhere to the specified criteria while also making sense within its context.\n",
      "\n",
      "\n",
      "\n",
      "Given the constraints above, the final choice leans towards choosing the version that closely resembles what was demonstrated earlier – focusing more on clarity rather than strict adherence to every guideline since there isn't a clear-cut winner\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Load LLaMA 8B pipeline (using bf16 to save memory)\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Get a specific row from test_df\n",
    "some_row = test_df.iloc[0]\n",
    "\n",
    "for i, question in enumerate(questions[3:]):\n",
    "    # Retrieve actual hypotheses for ASR correction\n",
    "    actual_hypotheses = list(some_row[['rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5']])\n",
    "    formatted_hypotheses = \"\\n\".join([f\"Hypothesis {i+1}: {hypothesis}\" for i, hypothesis in enumerate(actual_hypotheses)])\n",
    "\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    ### SYSTEM PROMPT ###\n",
    "    You are selecting the best ASR transcription.\n",
    "\n",
    "    RULES (Guidelines, But Selection is Mandatory):\n",
    "    - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
    "    - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
    "    - Ignore capitalization differences.\n",
    "    - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
    "    - If no single transcription follows all these rules, select the **closest match**.\n",
    "    - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
    "\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "    # Final formatted prompt\n",
    "    full_prompt = f\"{system_prompt}\\n### QUESTION ###\\n{question}\\n{formatted_hypotheses}\\n\\n### ANSWER ###\\nANSWER:\"\n",
    "\n",
    "    print(\"Prompt:\\n\")\n",
    "    print(full_prompt)\n",
    "\n",
    "    # Generate response using LLaMA pipeline\n",
    "    response = pipeline(\n",
    "        full_prompt,\n",
    "        max_new_tokens=256,  # Limit output length\n",
    "        min_length=5,\n",
    "        do_sample=False,  # Deterministic response\n",
    "        temperature=0.0,  # Avoid randomness\n",
    "        return_full_text=False,  # Prevents repeating the input prompt\n",
    "        repetition_penalty=1.2\n",
    "    )[0][\"generated_text\"].strip()\n",
    "    \n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(response if response else \"[ERROR: Blank Response]\")\n",
    "\n",
    "    # Free GPU memory after each run\n",
    "    del full_prompt, response\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
